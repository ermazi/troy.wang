<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>/bin/troy.wang</title>
    <link>https://troy.wang/docs/kubernetes/</link>
    <description>Recent content on /bin/troy.wang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://troy.wang/docs/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>argo-rollouts之流量管理添加traefik支持</title>
      <link>https://troy.wang/docs/kubernetes/posts/argo-rollouts-support-traefik/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://troy.wang/docs/kubernetes/posts/argo-rollouts-support-traefik/</guid>
      <description>argo-rollouts之流量管理添加traefik支持 背景 在使用argo-rollouts做金丝雀部署的时候，为了精准控制流量，使用它流量控制的特性，该特性需要结合ingress网关一起作用，目前官网仅支持Nginx,Istio,ALB，而笔者使用最多的是traefik。
最小化原则，最好在不改变现有基础设施的前提下用上argo-rollouts，所以被迫添加对traefik的支持。
traefik可行性验证 参考文档traefik分流，我们可以通过annotation的控制实现。
结合argo-rollouts以及traefik文档，我们需要准备的资源对象如下：
 app-rollout || Rollout app-stable-service || Service app-canary-service || Service app-stable-ingress || Ingress  traefik带权重控制的Ingress配置：
apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: traefik traefik.ingress.kubernetes.io/service-weights: | app-stable-service: 100% app-canary-service: 0% name: app-stable-ingress spec: rules: - http: paths: - backend: serviceName: app-stable-service servicePort: 80 path: / - backend: serviceName: app-canary-service servicePort: 80 path: / 对比nginx的流量控制，nginx采用两个ingress进行weighted负载均衡，而traefik采用一个ingress进行weighted负载均衡；所以在argo-rollouts的流量控制实现上，我们只需要去将目标ingress的annotation设置为上述目标ingress即可.  代码分析 type TrafficRoutingReconciler interface { Reconcile(desiredWeight int32) error Type() string } 这是流量控制的接口，我们只需要实现Reconcile方法和Type即可，传入目标权重值，将权重渲染到对应的ingress资源模板中应用即可。</description>
    </item>
    
    <item>
      <title>k8s灰度发布神器之argo-rollouts</title>
      <link>https://troy.wang/docs/kubernetes/posts/argo-rollouts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://troy.wang/docs/kubernetes/posts/argo-rollouts/</guid>
      <description>k8s灰度发布工具之argo-rollouts  Kubernetes默认的Deployment资源支持两种发布策略：RollingUpdate和Recreate，这两个恶略可以满足大部分发布场景；但是针对于各种大规模的生产场景，需要额外的发布策略，比如蓝绿部署或者金丝雀部署。为了满足改需求，argo-rollouts应运而生。
 背景 Recreate策略毫无平滑可言，通常用于仅可运行单副本的有状态应用/job。
RollingUpdate策略，在完善的Pod生命周期前提下，能做到平滑。
在生产持续部署过程中，多多少少会因为一些纰漏，导致新上线的应用出现问题；如果使用RollingUpdate，一旦下发指令，不可中断，直接影响全部流量，影响过大。
要解决/缓解该问题，有几个努力方向：
 尽可能完善的测试，减少问题 使用蓝绿部署，在新版本集中进行完善的测试；确认没问题后，切换流量到新版本 使用金丝雀部署，先更新部分副本，然后使用生产小流量（比如1%）进行验证，确认没问题之后，再缓慢放开，5% 20% 50% 100%  argo-rollouts咋工作的 跟Deployment对象类似，Rollout对象管理ReplicaSets。Rollout的spec.strategy字段决定发布如何从老的replicaSet变为新的replicaSet，期间可暂停，可继续，可发布更新的版本。假设三个阶段，分别对应三个replicaSets：stable、new、newer，当stable -&amp;gt; new时，当前处于desiredWeight/Paused状态：
 如果newer发布，那么new -&amp;gt; 0，newer -&amp;gt; desiredWeight，stable -&amp;gt; newer 如果abort操作，那么new -&amp;gt; 0，stable -&amp;gt; max 如果promote操作，那么new -&amp;gt; max, stable -&amp;gt;0  argo-rollouts使用场景  用户想在生产环境基于新版本做最后一分钟的功能测试，那么可以使用蓝绿部署。本质就是两个Service的切换 用户想在线上流量进来之前，需要做一系列准备，比如预热，初始化等操作，那么可以使用蓝绿部署。在绿版本上完成相应的操作，然后再切换接收线上流量 用户想在生产环境使用小流量验证一段时间，确认稳定运行，没有Bug之后，再全量发布，那么可以使用金丝雀部署。小流量发布后，暂停验证，查看日志，观测应用指标，然后决定是回滚还是继续推进。 用户想慢慢的放进生产流量，那么可以使用金丝雀部署。定义多steps，慢慢调大权重 用户任性，说我不想用任何策略，就想滚动更新，那么也可以使用Rollout，将steps置空即可。  部署 集群级别部署argo rollouts $ kubectl apply -n argo-rollouts -f https://raw.githubusercontent.com/argoproj/argo-rollouts/stable/manifests/install.yaml 官方文档讲得挺详细的，自行摸索吧  安装kubectl插件 curl -LO https://github.com/argoproj/argo-rollouts/releases/latest/download/kubectl-argo-rollouts-darwin-amd64 chmod +x ./kubectl-argo-rollouts-darwin-amd64 sudo mv ./kubectl-argo-rollouts-darwin-amd64 /usr/local/bin/kubectl-argo-rollouts kubectl argo rollouts version 附录 常用命令 查看 kubectl argo rollouts list rollouts 中断回滚 kubectl argo rollouts abort app-rollout 继续发布 kubectl argo rollouts promote app-rollout  </description>
    </item>
    
    <item>
      <title>在kubernetes生产环境coredns</title>
      <link>https://troy.wang/docs/kubernetes/posts/coredns-in-production/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://troy.wang/docs/kubernetes/posts/coredns-in-production/</guid>
      <description>在kubernetes生产环境coredns .:53 { errors log health reload kubernetes cluster.local. in-addr.arpa ip6.arpa { pods insecure upstream 10.40.0.53 10.40.0.54 fallthrough in-addr.arpa ip6.arpa } autopath @kubernetes prometheus :9153 forward . 10.40.0.53 10.40.0.54 cache 30 } </description>
    </item>
    
    <item>
      <title>在ubuntu上运行microk8s</title>
      <link>https://troy.wang/docs/kubernetes/posts/run-microk8s-on-ubuntu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://troy.wang/docs/kubernetes/posts/run-microk8s-on-ubuntu/</guid>
      <description>在ubuntu上运行microk8s 不可抗拒的理由 Reliable, fast, small, upstream.
  Fast install Get a full Kubernetes system running in under 60 seconds.
  Secure Runs safely on your laptop with state of the art isolation.
  Upstream CNCF binaries delivered to your laptop, with updates and upgrades.
  Complete Includes a docker registry so you can make containers, push them, and deploy them all on your laptop.
  Featureful Cool things you probably want to try on a small, standard K8s are all built-in.</description>
    </item>
    
  </channel>
</rss>